---
title: "John Hopkins Practical Machine Learning Report"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
The project for this course is to use machine learning to predict the manner in which a group did excercise. The following objectives are to be met as part of this report:
* Describe how you built your model
* How you used cross validation
* Why you made the choices you did

### Project Setup
First, include required libraries, create folders, and download files

```{r libraries, cache=TRUE, message=FALSE}
# Include libraries
library(caret)
```

```{r folders, cache=TRUE}
# Create data folder if it does not exist
dataFolder = "data"
if (!file.exists(dataFolder)) {
        dir.create(dataFolder)
}
```

```{r files, cache=TRUE}
# Download test files if they do not exist
trainInput <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testInput <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainDisk <- file.path(dataFolder,"training.csv")
testDisk <- file.path(dataFolder, "test.csv")

if (!file.exists(trainDisk)) {
        download.file(trainInput, destfile = trainDisk, method = "curl")
}

if (!file.exists(testDisk)) {
        download.file(testInput, destfile = testDisk, method = "curl")
}
```

### Prepare Data
Next we need to load, clean, and partition our data sets

```{r load, cache=TRUE}
# Read the downloaded csv files into data frames
trainCsv <- read.csv(trainDisk)
testCsv <- read.csv(testDisk)

# Next I used View(trainCsv) and I see lots of NA Values. Lets remove those first.
trainCsv <- trainCsv[, colSums(is.na(trainCsv)) == 0]
testCsv <-  testCsv[, colSums(is.na(testCsv)) == 0]

# Again I used View(trainCsv) and there are some obvious columns that are not useful
dropCol <- c("user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")
trainCsv <- trainCsv[, -which(names(trainCsv) %in% dropCol)]
testCsv <- testCsv[, -which(names(testCsv) %in% dropCol)]

# Even with this a Random Forest with 200 trees was still taking forever
# so lets turn our classe into a factor and set it aside for a moment
trainClasse <- as.factor(trainCsv$classe)

# Next lets strip out everything non-numeric
trainCsv <- trainCsv[, sapply(trainCsv, is.numeric)]
testCsv <- testCsv[, sapply(testCsv, is.numeric)]

# Then put our classe back
trainCsv$classe <- trainClasse

# Finally partition the data
set.seed(19622)
inTrain <- createDataPartition(trainCsv$classe, p = 3/4)[[1]]
training <- trainCsv[inTrain,]
testing <- trainCsv[-inTrain,]
```

## Model Building
Next we are going to build three different models and then combine the models and see what accuracy we get from each scenario

```{r models, cache=TRUE}
# Random Forest
mod_rf <- train(classe ~ ., data = training, method = "rf")
# Boosted
mod_gbm <- train(classe ~ ., data = training, method = "gbm", verbose=FALSE)
# Latent allocation
mod_lda <- train(classe ~ ., data = training, method = "lda")

# Run predictors
pred_rf <- predict(mod_rf, testing)
pred_gbm <- predict(mod_gbm, testing)
pred_lda <- predict(mod_lda, testing)

# Combined
predDF <- data.frame(pred_rf, pred_gbm, pred_lda, classe = testing$classe)
combModFit <- train(classe ~ ., method = "rf", data = predDF)
combPred <- predict(combModFit, predDF)
```

### Model Comparisons
Now that our models are built, we can compare the results.


```{r comparisons, cache=TRUE}
results <- resamples(list(rf=mod_rf, gbm=mod_gbm, lda=mod_lda, comb=combModFit))
summary(results)
```

As you can see, the predicted accuracy of Random Forest and Boosted are nearly identical. By combining all three, we can get an insignificantly higher accuracy.

Here you can see the results as a box plot:

```{r boxplot, cache=TRUE}
bwplot(results)
```

Here are the results as a dot plot

```{r dotplot, cache=TRUE}
dotplot(results)
```

### Model Selection
As the results of Random Forest, Boosted, and Combined were all above 99%, I am going to select the Random Forest model.  Let's go ahead and re-build the model with five fold cross validation and a larger forest.

```{r sample, cache=TRUE}
# Cross validation
cv <- trainControl(method="cv", 5)
mod_rf <- train(classe ~., data=training, method="rf", trControl=cv, ntree=250)
# View results
mod_rf
```

Next, let's estimate against the validation set.
```{r estimate, cache=TRUE}
pred_rf <- predict(mod_rf, testing)
confusionMatrix(testing$classe, pred_rf)
```

Finally, we can see the overall estimated accuracy for the Random Forest model:
```{r accuracy, cache=TRUE}
accuracy <- postResample(pred_rf, testing$classe)
accuracy
```

## Summary
In conclusion, we went with the Random Forest model because:
* It gave nearly the same accuracy as Boosted
* Performed much faster than building and combinding with the other algorithims
* With estimated accuracy of 99%, there is little need for improvement

Our final Random Forest model was built using five fold cross validation.



